{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载词向量模型\n",
    "word_vector_dict = KeyedVectors.load_word2vec_format(\n",
    "    \"./Dataset/wiki_word2vec_50.bin\", binary=True\n",
    ")\n",
    "\n",
    "# 确定词向量的维度\n",
    "vector_size = word_vector_dict.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件中读取数据\n",
    "def load_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        label = torch.tensor(float(parts[0]))\n",
    "        words = parts[1:]\n",
    "\n",
    "        word_vectors = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in word_vector_dict:\n",
    "                word_vector = word_vector_dict[word]\n",
    "                word_vectors.append(word_vector)\n",
    "\n",
    "        word_vectors = np.array(word_vectors)\n",
    "\n",
    "        if len(word_vectors) > 0:\n",
    "            sentence_vector = torch.tensor(word_vectors).view(-1, vector_size)\n",
    "            sentences.append(sentence_vector)\n",
    "            labels.append(label)\n",
    "        else:\n",
    "            print(f\"Empty sentence: {line}\")\n",
    "\n",
    "    return {\n",
    "        \"sentences\": sentences,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty sentence: 0\t鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "train_data = load_data(\"./Dataset/train.txt\")\n",
    "valid_data = load_data(\"./Dataset/validation.txt\")\n",
    "test_data = load_data(\"./Dataset/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将句子填充到相同长度\n",
    "def unify_columns(matrix, unified_length):\n",
    "    col_length, row_length = matrix.size()\n",
    "    if col_length > unified_length:\n",
    "        new_matrix = matrix[:unified_length, :]\n",
    "    elif col_length < unified_length:\n",
    "        padding = torch.zeros(unified_length - col_length, row_length)\n",
    "        new_matrix = torch.cat((padding, matrix), dim=0)\n",
    "    else:\n",
    "        new_matrix = matrix\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# 定义数据集类\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, sentences_len):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.sentence_len = sentences_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.sentences[index].float()\n",
    "        label = self.labels[index]\n",
    "        # 填充句子到最大长度\n",
    "        output_matrix = unify_columns(sentence, self.sentence_len).requires_grad_()\n",
    "        return output_matrix, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 19997\n",
      "验证集大小: 5629\n",
      "测试集大小: 369\n"
     ]
    }
   ],
   "source": [
    "# 创建Dataset对象\n",
    "SENTENCE_LEN = 64\n",
    "train_dataset = SentimentDataset(\n",
    "    train_data[\"sentences\"], train_data[\"labels\"], sentences_len=SENTENCE_LEN\n",
    ")\n",
    "valid_dataset = SentimentDataset(\n",
    "    valid_data[\"sentences\"], valid_data[\"labels\"], sentences_len=SENTENCE_LEN\n",
    ")\n",
    "test_dataset = SentimentDataset(\n",
    "    test_data[\"sentences\"], test_data[\"labels\"], sentences_len=SENTENCE_LEN\n",
    ")\n",
    "\n",
    "print(\"训练集大小:\", len(train_dataset))\n",
    "print(\"验证集大小:\", len(valid_dataset))\n",
    "print(\"测试集大小:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 创建 DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN模型\n",
    "class RNNSentimentClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_layers, dropout):\n",
    "        super(RNNSentimentClassifier, self).__init__()\n",
    "\n",
    "        # LSTM层\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # 初始化LSTM层的权重和偏置\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                init.orthogonal_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                init.zeros_(param.data)\n",
    "\n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "        # 初始化全连接层的权重和偏置\n",
    "        init.orthogonal_(self.fc.weight)\n",
    "        init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        # LSTM层的输出\n",
    "        lstm_output, (hidden, cell) = self.lstm(text)\n",
    "\n",
    "        # 使用最后一个时间步的输出\n",
    "        last_hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "\n",
    "        # 应用全连接层\n",
    "        logits = self.fc(last_hidden)\n",
    "\n",
    "        # 使用sigmoid激活函数\n",
    "        probs = F.sigmoid(logits)\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNSentimentClassifier(\n",
      "  (lstm): LSTM(50, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 超参数\n",
    "EMBEDDING_DIM = vector_size\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 1\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# 初始化模型\n",
    "model = RNNSentimentClassifier(\n",
    "    EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, NUM_LAYERS, DROPOUT\n",
    ")\n",
    "\n",
    "# 查看模型结构\n",
    "print(model)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24, Train Loss: 0.6932, Valid Loss: 0.6898, Train Accuracy: 0.5008, Valid Accuracy: 0.5014\n",
      "Epoch 2/24, Train Loss: 0.6766, Valid Loss: 0.6797, Train Accuracy: 0.5705, Valid Accuracy: 0.5523\n",
      "Epoch 3/24, Train Loss: 0.6839, Valid Loss: 0.6898, Train Accuracy: 0.5350, Valid Accuracy: 0.5100\n",
      "Epoch 4/24, Train Loss: 0.6895, Valid Loss: 0.6894, Train Accuracy: 0.5110, Valid Accuracy: 0.5111\n",
      "Epoch 5/24, Train Loss: 0.6891, Valid Loss: 0.6890, Train Accuracy: 0.5122, Valid Accuracy: 0.5127\n",
      "Epoch 6/24, Train Loss: 0.6829, Valid Loss: 0.6823, Train Accuracy: 0.5521, Valid Accuracy: 0.6090\n",
      "Epoch 7/24, Train Loss: 0.6771, Valid Loss: 0.6483, Train Accuracy: 0.5681, Valid Accuracy: 0.6390\n",
      "Epoch 8/24, Train Loss: 0.6439, Valid Loss: 0.6221, Train Accuracy: 0.6771, Valid Accuracy: 0.7395\n",
      "Epoch 9/24, Train Loss: 0.6267, Valid Loss: 0.6269, Train Accuracy: 0.7200, Valid Accuracy: 0.7574\n",
      "Epoch 10/24, Train Loss: 0.6169, Valid Loss: 0.6161, Train Accuracy: 0.7500, Valid Accuracy: 0.7328\n",
      "Epoch 11/24, Train Loss: 0.6146, Valid Loss: 0.6159, Train Accuracy: 0.7497, Valid Accuracy: 0.7303\n",
      "Epoch 12/24, Train Loss: 0.6114, Valid Loss: 0.6113, Train Accuracy: 0.7590, Valid Accuracy: 0.7480\n",
      "Epoch 13/24, Train Loss: 0.6068, Valid Loss: 0.6068, Train Accuracy: 0.7678, Valid Accuracy: 0.7641\n",
      "Epoch 14/24, Train Loss: 0.6032, Valid Loss: 0.6061, Train Accuracy: 0.7762, Valid Accuracy: 0.7815\n",
      "Epoch 15/24, Train Loss: 0.6005, Valid Loss: 0.6198, Train Accuracy: 0.7852, Valid Accuracy: 0.7076\n",
      "Epoch 16/24, Train Loss: 0.5996, Valid Loss: 0.6019, Train Accuracy: 0.7855, Valid Accuracy: 0.7784\n",
      "Epoch 17/24, Train Loss: 0.5954, Valid Loss: 0.5998, Train Accuracy: 0.7939, Valid Accuracy: 0.7791\n",
      "Epoch 18/24, Train Loss: 0.5922, Valid Loss: 0.6070, Train Accuracy: 0.8017, Valid Accuracy: 0.7821\n",
      "Epoch 19/24, Train Loss: 0.5891, Valid Loss: 0.5985, Train Accuracy: 0.8074, Valid Accuracy: 0.7864\n",
      "Epoch 20/24, Train Loss: 0.5891, Valid Loss: 0.6013, Train Accuracy: 0.8101, Valid Accuracy: 0.7650\n",
      "Epoch 21/24, Train Loss: 0.5862, Valid Loss: 0.5968, Train Accuracy: 0.8153, Valid Accuracy: 0.7822\n",
      "Epoch 22/24, Train Loss: 0.5821, Valid Loss: 0.5981, Train Accuracy: 0.8244, Valid Accuracy: 0.7801\n",
      "Epoch 23/24, Train Loss: 0.5821, Valid Loss: 0.5964, Train Accuracy: 0.8254, Valid Accuracy: 0.7806\n",
      "Epoch 24/24, Train Loss: 0.5791, Valid Loss: 0.5945, Train Accuracy: 0.8308, Valid Accuracy: 0.8065\n"
     ]
    }
   ],
   "source": [
    "# 定义准确率计算函数\n",
    "def accuracy(logits, labels):\n",
    "    labels = labels.view(-1, 1)\n",
    "    y_pred = logits >= 0.5\n",
    "    y = labels == 1\n",
    "    correct_predictions = (y_pred == y).float()\n",
    "    correct_predictions = correct_predictions.view(-1)\n",
    "    accuracy = correct_predictions.sum() / len(labels)\n",
    "    return accuracy\n",
    "\n",
    "def train_epoch(model, data_loader, loss_function, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    for sentences, labels in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(sentences)\n",
    "        loss = loss_function(logits, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        batch_accuracy = accuracy(logits, labels)\n",
    "        total_accuracy += batch_accuracy.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    average_accuracy = total_accuracy / len(data_loader)\n",
    "    return average_loss, average_accuracy\n",
    "\n",
    "\n",
    "def valid_epoch(model, data_loader, loss_function):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for sentences, labels in data_loader:\n",
    "            logits = model(sentences)\n",
    "            loss = loss_function(logits, labels.unsqueeze(1))\n",
    "            total_loss += loss.item()\n",
    "            batch_accuracy = accuracy(logits, labels)\n",
    "            total_accuracy += batch_accuracy.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    average_accuracy = total_accuracy / len(data_loader)\n",
    "    return average_loss, average_accuracy\n",
    "\n",
    "\n",
    "num_epochs = 24\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train_epoch(\n",
    "        model, train_loader, loss_function, optimizer\n",
    "    )\n",
    "    valid_loss, valid_accuracy = valid_epoch(model, valid_loader, loss_function)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, \"\n",
    "        f\"Train Accuracy: {train_accuracy:.4f}, Valid Accuracy: {valid_accuracy:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6151, Test Accuracy: 0.7975\n"
     ]
    }
   ],
   "source": [
    "# 测试循环以评估模型性能\n",
    "def test_epoch(model, data_loader, loss_function):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for sentences, labels in data_loader:\n",
    "            logits = model(sentences)\n",
    "            loss = loss_function(logits, labels.unsqueeze(1))\n",
    "            total_loss += loss.item()\n",
    "            batch_accuracy = accuracy(logits, labels)\n",
    "            total_accuracy += batch_accuracy.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    average_accuracy = total_accuracy / len(data_loader)\n",
    "    return average_loss, average_accuracy\n",
    "\n",
    "\n",
    "# 在训练完成后，评估模型在测试集上的性能\n",
    "test_loss, test_accuracy = test_epoch(model, test_loader, loss_function)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model.state_dict(), \"rnn_sentiment_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score(probs, labels):\n",
    "    probs = probs.view(-1)\n",
    "    y_true = list(labels == 1)\n",
    "    for i in range(len(y_true)):\n",
    "        y_true[i] = y_true[i].item()\n",
    "    print(y_true)\n",
    "    y_pred = list(probs >= 0.5)\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred[i] = y_pred[i].item()\n",
    "    print(y_pred)\n",
    "    true_positives = sum([1 for y, p in zip(y_true, y_pred) if y == True and p == True])\n",
    "    false_positives = sum(\n",
    "        [1 for y, p in zip(y_true, y_pred) if y == False and p == True]\n",
    "    )\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall_score(probs, labels):\n",
    "    probs = probs.view(-1)\n",
    "    y_true = list(labels == 1)\n",
    "    for i in range(len(y_true)):\n",
    "        y_true[i] = y_true[i].item()\n",
    "\n",
    "    y_pred = list(probs >= 0.5)\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred[i] = y_pred[i].item()\n",
    "\n",
    "    true_positives = sum([1 for y, p in zip(y_true, y_pred) if y == True and p == True])\n",
    "    false_negatives = sum(\n",
    "        [1 for y, p in zip(y_true, y_pred) if y == True and p == False]\n",
    "    )\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNSentimentClassifier(\n",
       "  (lstm): LSTM(50, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型\n",
    "model.load_state_dict(torch.load(\"rnn_sentiment_model.pth\"))\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "[True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, False, False, True, True, True, True, True, True, False, True, False, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, False, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, False, True, True, True, False, False, False, True, False, False, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, False, True, True, True, False, True, False, False, True, True, False, True, False, True, True, True, True, False, True, False, True, True, True, False, True, False, False, True, False, False, True, True, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, True, True, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Average Loss: 0.5786\n",
      "Average Accuracy: 0.8293\n",
      "Average Precision: 0.8690\n",
      "Average Recall: 0.7807\n",
      "Average F1 Score: 0.8225\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "total_loss = 0\n",
    "total_accuracy = 0\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "with torch.no_grad():  # 关闭梯度计算\n",
    "    for sentences, labels in test_loader:\n",
    "        probs = model(sentences)\n",
    "\n",
    "        batch_loss = loss_function(probs, labels.unsqueeze(1))\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "        batch_accuracy = accuracy(probs, labels)\n",
    "        total_accuracy += batch_accuracy.item()\n",
    "\n",
    "        # 计算精确率、召回率和F-score\n",
    "        batch_precision = precision_score(probs, labels)\n",
    "        batch_recall = recall_score(probs, labels)\n",
    "\n",
    "        total_precision += batch_precision\n",
    "        total_recall += batch_recall\n",
    "\n",
    "average_loss = total_loss / len(test_loader)\n",
    "average_accuracy = total_accuracy / len(test_loader)\n",
    "average_precision = total_precision / len(test_loader)\n",
    "average_recall = total_recall / len(test_loader)\n",
    "average_f1 = 2 * ((average_precision * average_recall) / (average_precision + average_recall))\n",
    "\n",
    "print(f\"Average Loss: {average_loss:.4f}\")\n",
    "print(f\"Average Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average Precision: {average_precision:.4f}\")\n",
    "print(f\"Average Recall: {average_recall:.4f}\")\n",
    "print(f\"Average F1 Score: {average_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
