{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载词向量模型\n",
    "word_vector_dict = KeyedVectors.load_word2vec_format(\n",
    "    \"./Dataset/wiki_word2vec_50.bin\", binary=True\n",
    ")\n",
    "\n",
    "# 确定词向量的维度\n",
    "vector_size = word_vector_dict.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件中读取数据\n",
    "def load_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        label = torch.tensor(float(parts[0]))\n",
    "        words = parts[1:]\n",
    "\n",
    "        word_vectors = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in word_vector_dict:\n",
    "                word_vector = word_vector_dict[word]\n",
    "                word_vectors.append(word_vector)\n",
    "\n",
    "        word_vectors = np.array(word_vectors)\n",
    "\n",
    "        if len(word_vectors) > 0:\n",
    "            sentence_vector = torch.tensor(word_vectors).view(-1, vector_size)\n",
    "            sentences.append(sentence_vector)\n",
    "            labels.append(label)\n",
    "        else:\n",
    "            print(f\"Empty sentence: {line}\")\n",
    "\n",
    "    return {\n",
    "        \"sentences\": sentences,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty sentence: 0\t鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆 鸟爆\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "train_data = load_data(\"./Dataset/train.txt\")\n",
    "valid_data = load_data(\"./Dataset/validation.txt\")\n",
    "test_data = load_data(\"./Dataset/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大训练集句子长度： 648\n",
      "最大验证集句子长度： 113\n",
      "最大测试集句子长度： 81\n",
      "最大句子长度： 648\n",
      "最小训练集句子长度： 10\n",
      "最小验证集句子长度： 11\n",
      "最小测试集句子长度： 19\n",
      "最小句子长度： 10\n",
      "平均训练集句子长度： 43.42051307696154\n",
      "平均验证集句子长度： 43.61840468999822\n",
      "平均测试集句子长度： 43.8780487804878\n",
      "平均句子长度： 43.63898884914919\n"
     ]
    }
   ],
   "source": [
    "# 句子长度的 summary\n",
    "max_train_len = max([len(sentence) for sentence in train_data[\"sentences\"]])\n",
    "max_valid_len = max([len(sentence) for sentence in valid_data[\"sentences\"]])\n",
    "max_test_len = max([len(sentence) for sentence in test_data[\"sentences\"]])\n",
    "max_len = max(max_train_len, max_valid_len, max_test_len)\n",
    "print(\"最大训练集句子长度：\", max_train_len)\n",
    "print(\"最大验证集句子长度：\", max_valid_len)\n",
    "print(\"最大测试集句子长度：\", max_test_len)\n",
    "print(\"最大句子长度：\", max_len)\n",
    "\n",
    "min_train_len = min([len(sentence) for sentence in train_data[\"sentences\"]])\n",
    "min_valid_len = min([len(sentence) for sentence in valid_data[\"sentences\"]])\n",
    "min_test_len = min([len(sentence) for sentence in test_data[\"sentences\"]])\n",
    "min_len = min(min_train_len, min_valid_len, min_test_len)\n",
    "print(\"最小训练集句子长度：\", min_train_len)\n",
    "print(\"最小验证集句子长度：\", min_valid_len)\n",
    "print(\"最小测试集句子长度：\", min_test_len)\n",
    "print(\"最小句子长度：\", min_len)\n",
    "\n",
    "average_train_len = np.mean([len(sentence) for sentence in train_data[\"sentences\"]])\n",
    "average_valid_len = np.mean([len(sentence) for sentence in valid_data[\"sentences\"]])\n",
    "average_test_len = np.mean([len(sentence) for sentence in test_data[\"sentences\"]])\n",
    "average_len = np.mean([average_train_len, average_valid_len, average_test_len])\n",
    "print(\"平均训练集句子长度：\", average_train_len)\n",
    "print(\"平均验证集句子长度：\", average_valid_len)\n",
    "print(\"平均测试集句子长度：\", average_test_len)\n",
    "print(\"平均句子长度：\", average_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将句子填充到相同长度\n",
    "def unify_columns(matrix, unified_length):\n",
    "    col_length, row_length = matrix.size()\n",
    "    if col_length > unified_length:\n",
    "        new_matrix = matrix[:unified_length, :]\n",
    "    elif col_length < unified_length:\n",
    "        padding = torch.zeros(unified_length - col_length, row_length)\n",
    "        new_matrix = torch.cat((matrix, padding), dim=0)\n",
    "    else:\n",
    "        new_matrix = matrix\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 定义数据集类\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, sentences_len):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.sentence_len = sentences_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.sentences[index].float()\n",
    "        label = self.labels[index]\n",
    "        # 填充句子到最大长度\n",
    "        # 转换形状以适配卷积层: (batch_size, embedding_dim, sentence_length)\n",
    "        output_matrix = unify_columns(sentence, self.sentence_len).permute(1, 0).requires_grad_()\n",
    "        return output_matrix, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 19997\n",
      "验证集大小: 5629\n",
      "测试集大小: 369\n"
     ]
    }
   ],
   "source": [
    "# 创建Dataset对象\n",
    "SENTENCE_LEN = 80\n",
    "train_dataset = SentimentDataset(\n",
    "    train_data[\"sentences\"], train_data[\"labels\"], sentences_len=SENTENCE_LEN\n",
    ")\n",
    "valid_dataset = SentimentDataset(\n",
    "    valid_data[\"sentences\"], valid_data[\"labels\"], sentences_len=SENTENCE_LEN\n",
    ")\n",
    "test_dataset = SentimentDataset(\n",
    "    test_data[\"sentences\"], test_data[\"labels\"], sentences_len=SENTENCE_LEN\n",
    ")\n",
    "\n",
    "print(\"训练集大小:\", len(train_dataset))\n",
    "print(\"验证集大小:\", len(valid_dataset))\n",
    "print(\"测试集大小:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 创建DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN模型\n",
    "class CNNSentimentClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_filters, filter_sizes, output_dim, dropout):\n",
    "        super(CNNSentimentClassifier, self).__init__()\n",
    "\n",
    "        # 卷积层列表\n",
    "        self.conv_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(\n",
    "                    in_channels=embedding_dim,\n",
    "                    out_channels=num_filters,\n",
    "                    kernel_size=size,\n",
    "                )\n",
    "                for size in filter_sizes\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), output_dim)\n",
    "\n",
    "        # 初始化卷积层\n",
    "        for conv_layer in self.conv_layers:\n",
    "            init.kaiming_normal_(conv_layer.weight, nonlinearity=\"relu\")\n",
    "            init.zeros_(conv_layer.bias)\n",
    "\n",
    "        # 初始化全连接层\n",
    "        init.xavier_normal_(self.fc.weight)\n",
    "        init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        # 保存所有卷积层的输出\n",
    "        conv_outputs = []\n",
    "\n",
    "        # 遍历所有卷积层\n",
    "        for conv_layer in self.conv_layers:\n",
    "            # 应用卷积层\n",
    "            conv_output = conv_layer(text)\n",
    "            # 应用ReLU激活函数\n",
    "            conv_output = F.relu(conv_output)\n",
    "            # 应用最大池化层\n",
    "            conv_output, _ = torch.max(conv_output, dim=2)\n",
    "            # 保存卷积层的输出\n",
    "            conv_outputs.append(conv_output)\n",
    "\n",
    "        # 将所有卷积层的输出拼接起来\n",
    "        concat_output = torch.cat(conv_outputs, dim=1)\n",
    "\n",
    "        # 应用Dropout\n",
    "        concat_output = self.dropout(concat_output)\n",
    "\n",
    "        # 应用全连接层\n",
    "        logits = self.fc(concat_output)\n",
    "\n",
    "        # 使用sigmoid激活函数\n",
    "        probs = F.sigmoid(logits)\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNSentimentClassifier(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv1d(50, 100, kernel_size=(3,), stride=(1,))\n",
      "    (1): Conv1d(50, 100, kernel_size=(5,), stride=(1,))\n",
      "    (2): Conv1d(50, 100, kernel_size=(7,), stride=(1,))\n",
      "    (3): Conv1d(50, 100, kernel_size=(9,), stride=(1,))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=400, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 超参数\n",
    "EMBEDDING_DIM = vector_size\n",
    "NUM_FILTERS = 100\n",
    "FILTER_SIZES = [3, 5, 7, 9]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# 初始化模型\n",
    "model = CNNSentimentClassifier(\n",
    "    EMBEDDING_DIM, NUM_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT\n",
    ")\n",
    "\n",
    "# 查看模型结构\n",
    "print(model)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练循环\n",
    "def train_epoch(model, data_loader, loss_function, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for sentences, labels in data_loader:\n",
    "        # 清除梯度\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向传播\n",
    "        probs = model(sentences)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = loss_function(probs, labels.unsqueeze(1))\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "\n",
    "        # 更新权重\n",
    "        optimizer.step()\n",
    "\n",
    "        # 累加损失\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "# 定义验证循环\n",
    "def valid_epoch(model, data_loader, loss_function):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for sentences, labels in data_loader:\n",
    "            # 前向传播\n",
    "            probs = model(sentences)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = loss_function(probs, labels.unsqueeze(1))\n",
    "\n",
    "            # 累加损失\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义准确率计算函数\n",
    "def accuracy(probs, labels):\n",
    "    labels = labels.view(-1, 1)\n",
    "    y_pred = probs >= 0.5\n",
    "    y = labels == 1\n",
    "    correct_predictions = (y_pred == y).float()\n",
    "    correct_predictions = correct_predictions.view(-1)\n",
    "    accuracy = correct_predictions.sum() / len(labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6936, Valid Loss: 0.6931, Train Accuracy: 0.5006, Valid Accuracy: 0.5004\n",
      "Epoch 2/10, Train Loss: 0.6398, Valid Loss: 0.6106, Train Accuracy: 0.6790, Valid Accuracy: 0.7586\n",
      "Epoch 3/10, Train Loss: 0.6075, Valid Loss: 0.6073, Train Accuracy: 0.7674, Valid Accuracy: 0.7798\n",
      "Epoch 4/10, Train Loss: 0.5995, Valid Loss: 0.6053, Train Accuracy: 0.7859, Valid Accuracy: 0.7886\n",
      "Epoch 5/10, Train Loss: 0.5916, Valid Loss: 0.6007, Train Accuracy: 0.8057, Valid Accuracy: 0.7896\n",
      "Epoch 6/10, Train Loss: 0.5847, Valid Loss: 0.5987, Train Accuracy: 0.8216, Valid Accuracy: 0.7903\n",
      "Epoch 7/10, Train Loss: 0.5771, Valid Loss: 0.5970, Train Accuracy: 0.8407, Valid Accuracy: 0.7874\n",
      "Epoch 8/10, Train Loss: 0.5719, Valid Loss: 0.5970, Train Accuracy: 0.8525, Valid Accuracy: 0.8071\n",
      "Epoch 9/10, Train Loss: 0.5663, Valid Loss: 0.5959, Train Accuracy: 0.8646, Valid Accuracy: 0.8096\n",
      "Epoch 10/10, Train Loss: 0.5615, Valid Loss: 0.5975, Train Accuracy: 0.8769, Valid Accuracy: 0.8158\n"
     ]
    }
   ],
   "source": [
    "# 修改训练循环以包括准确率\n",
    "def train_epoch(model, data_loader, loss_function, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    for sentences, labels in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        probs = model(sentences)\n",
    "        loss = loss_function(probs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        batch_accuracy = accuracy(probs, labels)\n",
    "        total_accuracy += batch_accuracy.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    average_accuracy = total_accuracy / len(data_loader)\n",
    "    return average_loss, average_accuracy\n",
    "\n",
    "\n",
    "# 修改验证循环以包括准确率\n",
    "def valid_epoch(model, data_loader, loss_function):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for sentences, labels in data_loader:\n",
    "            probs = model(sentences)\n",
    "            loss = loss_function(probs, labels.unsqueeze(1))\n",
    "            total_loss += loss.item()\n",
    "            batch_accuracy = accuracy(probs, labels)\n",
    "            total_accuracy += batch_accuracy.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    average_accuracy = total_accuracy / len(data_loader)\n",
    "    return average_loss, average_accuracy\n",
    "\n",
    "\n",
    "# 开始训练，并打印准确率\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train_epoch(\n",
    "        model, train_loader, loss_function, optimizer\n",
    "    )\n",
    "    valid_loss, valid_accuracy = valid_epoch(model, valid_loader, loss_function)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, \"\n",
    "        f\"Train Accuracy: {train_accuracy:.4f}, Valid Accuracy: {valid_accuracy:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model.state_dict(), \"cnn_sentiment_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score(probs, labels):\n",
    "    probs = probs.view(-1)\n",
    "    y_true = list(labels == 1)\n",
    "    for i in range(len(y_true)):\n",
    "        y_true[i] = y_true[i].item()\n",
    "    print(y_true)\n",
    "    y_pred = list(probs >= 0.5)\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred[i] = y_pred[i].item()\n",
    "    print(y_pred)\n",
    "    true_positives = sum([1 for y, p in zip(y_true, y_pred) if y == True and p == True])\n",
    "    false_positives = sum(\n",
    "        [1 for y, p in zip(y_true, y_pred) if y == False and p == True]\n",
    "    )\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall_score(probs, labels):\n",
    "    probs = probs.view(-1)\n",
    "    y_true = list(labels == 1)\n",
    "    for i in range(len(y_true)):\n",
    "        y_true[i] = y_true[i].item()\n",
    "\n",
    "    y_pred = list(probs >= 0.5)\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred[i] = y_pred[i].item()\n",
    "\n",
    "    true_positives = sum([1 for y, p in zip(y_true, y_pred) if y == True and p == True])\n",
    "    false_negatives = sum(\n",
    "        [1 for y, p in zip(y_true, y_pred) if y == True and p == False]\n",
    "    )\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNSentimentClassifier(\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv1d(50, 100, kernel_size=(3,), stride=(1,))\n",
       "    (1): Conv1d(50, 100, kernel_size=(5,), stride=(1,))\n",
       "    (2): Conv1d(50, 100, kernel_size=(7,), stride=(1,))\n",
       "    (3): Conv1d(50, 100, kernel_size=(9,), stride=(1,))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=400, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型\n",
    "model.load_state_dict(torch.load(\"cnn_sentiment_model.pth\"))\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5927, Test Accuracy: 0.8154\n"
     ]
    }
   ],
   "source": [
    "# 测试循环以评估模型性能\n",
    "def test_epoch(model, data_loader, loss_function):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for sentences, labels in data_loader:\n",
    "            probs = model(sentences)\n",
    "            loss = loss_function(probs, labels.unsqueeze(1))\n",
    "            total_loss += loss.item()\n",
    "            batch_accuracy = accuracy(probs, labels)\n",
    "            total_accuracy += batch_accuracy.item()\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    average_accuracy = total_accuracy / len(data_loader)\n",
    "    return average_loss, average_accuracy\n",
    "\n",
    "# 在训练完成后，评估模型在测试集上的性能\n",
    "test_loss, test_accuracy = test_epoch(model, test_loader, loss_function)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "[True, False, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, False, False, False, True, True, True, True, True, True, False, False, True, True, False, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, False, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, False, True, True, True, False, False, False, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, False, True, True, False, True, True, False, True, False, True, True, True, True, True, False, False, True, True, True, False, True, False, False, False, False, False, True, True, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, True, False, True, False, True, False, True, False, False, False, False, False, False, False, True, True, False, True, False, False, False, False, False, True, True, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Average Loss: 0.5867\n",
      "Average Accuracy: 0.8103\n",
      "Average Precision: 0.8305\n",
      "Average Recall: 0.7861\n",
      "Average F1 Score: 0.8077\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "total_loss = 0\n",
    "total_accuracy = 0\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "with torch.no_grad():  # 关闭梯度计算\n",
    "    for sentences, labels in test_loader:\n",
    "        probs = model(sentences)\n",
    "\n",
    "        batch_loss = loss_function(probs, labels.unsqueeze(1))\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "        batch_accuracy = accuracy(probs, labels)\n",
    "        total_accuracy += batch_accuracy.item()\n",
    "\n",
    "        # 计算精确率、召回率和F-score\n",
    "        batch_precision = precision_score(probs, labels)\n",
    "        batch_recall = recall_score(probs, labels)\n",
    "\n",
    "        total_precision += batch_precision\n",
    "        total_recall += batch_recall\n",
    "\n",
    "average_loss = total_loss / len(test_loader)\n",
    "average_accuracy = total_accuracy / len(test_loader)\n",
    "average_precision = total_precision / len(test_loader)\n",
    "average_recall = total_recall / len(test_loader)\n",
    "average_f1 = 2 * ((average_precision * average_recall) / (average_precision + average_recall))\n",
    "\n",
    "print(f\"Average Loss: {average_loss:.4f}\")\n",
    "print(f\"Average Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average Precision: {average_precision:.4f}\")\n",
    "print(f\"Average Recall: {average_recall:.4f}\")\n",
    "print(f\"Average F1 Score: {average_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
